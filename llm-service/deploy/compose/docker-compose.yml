services:
  llm:
    image: llm-service
    restart: on-failure:5
    volumes:
      - ./models:/app/models
      - ./assets:/app/assets
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - PROJECT_ROOT=${PROJECT_ROOT}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.llm.rule=PathPrefix(`/llm-service/llm`)"
      - "traefik.http.routers.llm.entrypoints=api-t"
      - "traefik.http.services.llm.loadbalancer.server.port=8000"
    networks:
      - local_net
      - gateway_net


  vllm:
    image: vllm/vllm-openai:latest
    restart: always
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    ports:
      - "8001:8000"
    volumes:
      - ./models:/root/.cache/huggingface/hub/
    # command: >
    #   --model unsloth/Qwen2.5-3B-Instruct-bnb-4bit
    #   --dtype bfloat16
    #   --max-model-len 32768
    #   --gpu-memory-utilization 0.6
    #   --quantization bitsandbytes
    #   --load-format bitsandbytes
    #   --enable-lora
    #   --max-lora-rank 64
    #   --lora-modules vllm-QC-AI=Namtran0912/Qwen2.5-3B-Instruct-lora-adapter-v3
    # command: >
    #   --model unsloth/Llama-3.2-3B-Instruct
    #   --dtype bfloat16
    #   --max-model-len 32768
    #   --gpu-memory-utilization 0.9
    #   --quantization bitsandbytes
    #   --load-format bitsandbytes
    #   --enable-lora
    #   --max-lora-rank 64
    #   --lora-modules vllm-QC-AI=Namtran0912/Meta-Llama-3.2-3B-Instruct-lora-adapter-v4
    # command: >
    #   --model unsloth/Llama-3.2-3B-Instruct
    #   --dtype bfloat16
    #   --max-model-len 32768
    #   --gpu-memory-utilization 0.9
    #   --enable-lora
    #   --max-lora-rank 64
    #   --lora-modules vllm-QC-AI=Namtran0912/Meta-Llama-3.2-3B-Instruct-lora-adapter-v4
    command: >
      --model Qwen/Qwen2.5-3B-Instruct-AWQ
      --quantization awq_marlin
      --dtype bfloat16
      --max-model-len 32768
      --gpu-memory-utilization 0.6
      --enable-lora
      --max-lora-rank 64
      --lora-modules vllm-QC-AI=Namtran0912/Qwen2.5-3B-Instruct-lora-adapter-v4


    networks:
      - local_net
      - gateway_net
      - cloudflare_tunnel_net


#   ollama:
#     image: ollama/ollama
#     container_name: ${COMPOSE_PROJECT_NAME}_ollama
#     volumes:
#       - ollama:/root/.ollama
#       - ${PROJECT_ROOT}/models:/models
#       - ${PROJECT_ROOT}/modelfile:/modelfile
#     restart: on-failure:5
#     deploy:
#       resources:
#         reservations:
#           devices:
#             - driver: nvidia
#               count: all
#               capabilities: [gpu]
#     labels:
#       - "traefik.enable=true"
#       - "traefik.http.routers.ollama.rule=PathPrefix(`/llm-service/ollama`)"
#       - "traefik.http.routers.ollama.entrypoints=api-t"
#       - "traefik.http.middlewares.ollama-stripper.stripprefix.prefixes=/llm-service/ollama"
#       - "traefik.http.routers.ollama.middlewares=ollama-stripper"
#       - "traefik.http.services.ollama.loadbalancer.server.port=11434"
#     networks:
#       - gateway_net
#       - local_net

# volumes:
#   ollama:

networks:
  gateway_net:
    external: true
  cloudflare_tunnel_net:
    external: true
  local_net:
    driver: bridge